 âœ‹ Silence Bridging

Silence Bridging** is a web-based application that helps bridge communication between the Deaf and hearing communities through Tanzanian Sign Language (TSL) recognition. Using a webcam, the app captures hand landmarks in real-time and predicts the corresponding sign alphabet, providing both visual and audio feedback.

### ğŸŒ Live Demo

ğŸ”— [Open App](https://silencebridging-git-main-silencebridgings-projects.vercel.app/)

---

## ğŸš€ Features

- ğŸ“· Real-time hand landmark capture via webcam
- ğŸ§  Machine Learning model for sign recognition
- ğŸ”Š Speech output for predicted letters/words
- ğŸŒ Web-first experience (Next.js frontend + Python backend)
- ğŸ‡¹ğŸ‡¿ Focused on Tanzanian Sign Language (TSL)

---

## ğŸ› ï¸ Tech Stack

| Frontend | Backend | ML/AI        | Deployment |
|----------|---------|--------------|------------|
| Next.js  | FastAPI / Flask | TensorFlow / ONNX | Vercel + PythonAnywhere / Render |

---

## ğŸ“¦ Installation & Development

### 1. Clone the repository

```bash
git clone https://github.com/silencebridging/silencebridging.git
cd silencebridging
cd frontend
npm install
npm run dev
cd backend
pip install -r requirements.txt
uvicorn app:app --reload
Bridging Silence
Breaking Barriers, Building Connections
Bridging Silence is a web application that transforms communication between Deaf and Hearing communities through real-time AI tools, workshops, and inclusive designs. Our technology ensures no one is left out of the conversation.

<img alt="Bridging Silence" src="https://placeholder-for-project-screenshot.com/">
ğŸŒŸ Features
Sign-to-Sauti (Speech): Converts sign language gestures into audible speech in real-time
Sign-to-Text: Translates sign language into written text
Sauti-to-Sign: Converts spoken language into sign language visuals
Sign Language Learning: Interactive tools to learn sign language basics
ğŸš€ Technologies
Frontend: Next.js with React
Styling: TailwindCSS
Hand Tracking: MediaPipe
Real-time Processing: WebRTC for camera access
UI Components: Custom responsive components
ğŸ“‹ Prerequisites
Node.js (v16.0.0 or later)
npm or yarn
Modern web browser with camera access
ğŸ“± Usage
Sign-to-Sauti Translation
Navigate to the "Sign-to-Sauti" service page
Allow camera access when prompted
Position your hand in the camera view
Click "Translate" to begin real-time sign language translation
Use the zoom controls to adjust the view as needed
Camera Controls
Switch Camera: Toggle between front and back cameras
Zoom: Adjust zoom level (0.5x, 1x, 2x)
Fullscreen: Expand the camera view for better visibility
Pause/Resume: Temporarily pause translation
ğŸ§  How It Works
Bridging Silence uses advanced computer vision algorithms to:

Detect and track hand landmarks in real-time
Analyze hand gestures and finger positions
Match patterns to known sign language gestures
Convert recognized signs to text and speech
ğŸ¤ Contributing
Contributions are welcome! Please feel free to submit a Pull Request.

Fork the repository
Create your feature branch (git checkout -b feature/amazing-feature)
Commit your changes (git commit -m 'Add some amazing feature')
Push to the branch (git push origin feature/amazing-feature)
Open a Pull Request
ğŸ“„ License
This project is licensed under the MIT License - see the LICENSE file for details.

ğŸ“ Contact
Email: bridgingsilence@gmail.com Phone: +255 698959522
